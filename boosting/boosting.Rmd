---
title: "Boosting"
output: github_document
---

# Libraries, evaluation function, and clean data 

```{r}
library(tidyverse)
library(caret)
library(pROC)

# Build custom AUC function to extract AUC
# from the caret model object
eval_mod <- function(model, data) {
  pred <- predict(model, data)
  cm <- caret::confusionMatrix(pred, data$classes, positive="malignant")
  auc <- roc(data$classes,
             predict(model, data, type = "prob")[, "malignant"]) %>% auc()
  result <- c(cm$overall["Accuracy"],cm$byClass['Sensitivity'], cm$byClass['Specificity'], cm$byClass['F1'],AUC=auc)
  return(result)
}

bc_data <- readRDS("../EDA/bc_clean.RDS")
bc_data$classes <- as.factor(bc_data$classes)

```


```{r}
set.seed(2024)
index <- caret::createDataPartition(bc_data$classes, p = 0.7, list = FALSE)

train_data <- bc_data[index, ]
test_data  <- bc_data[-index, ]

train_data$classes %>% table(.)
```

```{r}
set.seed(2024)
ctrl <- trainControl(method = "repeatedcv", 
                      number = 5, 
                      repeats = 3,  
                      savePredictions = TRUE,
                      summaryFunction = twoClassSummary,
                      classProbs = TRUE)
```

```{r}
set.seed(2024)
forest_fit <- train(
  classes ~ .,
  data = train_data,                         
  method = "ranger",
  metric = "ROC",
  trControl = trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary),
  importance="impurity"
)
forest <- eval_mod(forest_fit,test_data)
```

```{r}
set.seed(2024)
gbm_fit <- train(classes ~ .,
                  data = train_data,
                  method = "gbm",
                  verbose = FALSE,
                  metric = "ROC",
                  trControl = ctrl)

gbm <- eval_mod(gbm_fit,test_data)
```

```{r}
modelLookup("xgbTree")

# nrounds: number of boosting iterations, M
# max_depth: maximum tree depth
# eta: shrinkage,  Î·
# gamma: minimum loss reduction
# colsamle_bytree: subsample ratio of columns
# min_child_weight: minimum size of instance weight
# substample: subsample percentage
# An alternative to tuneGrid
# tuneLength argument is used to control the number of combinations generated by this random tuning parameter search.
# 
# 
```

```{r}
set.seed(2024)

xgbGrid <-  expand.grid(nrounds = c(10, 10, 10), 
                        max_depth = c(1, 5, 1), 
                        gamma = c(0, 1, 0.1), eta = 0.3,
                          colsample_bytree=1, 
                          min_child_weight=1, subsample = 0.5)

xgboostT_fit <- train(classes ~ .,
                 data = train_data,
                 method = "xgbTree",
                 verbose = FALSE, tuneGrid = xgbGrid,
                 trControl = ctrl)


xgboost_tree <- eval_mod(xgboostT_fit,test_data)
```


```{r}
set.seed(2024)

xgboostL_fit <- train(classes ~ .,
                      data = train_data,
                      method = "xgbLinear",
                      verbose = FALSE,
                      tuneLength=5,
                      trControl = ctrl)

xgboost_linear <- eval_mod(xgboostL_fit,test_data)
```

```{r}
rbind(forest, gbm, xgboost_tree, xgboost_linear)
```

# Variable importance
Calculate variable importance using vip
```{r}
varImp(xgboostL_fit)

vip::vip(xgboostL_fit , num_features = 10) 

vip::vip(xgboostT_fit , num_features = 10) 
vip::vip(forest_fit , num_features = 10) 
```
